I"h<p>Following on from the <a href="https://tr8dr.github.io/MLHardProblem1/">prior post</a>, want to discuss the repercussions of the <strong>low signal / noise ratio</strong> and how it effects:</p>

<ul>
  <li>labeling / mis-labeling</li>
  <li>patterns unsupported by features</li>
</ul>

<p>How does this manifest and what might we do to ameliorate the issues it poses.</p>

<h2 id="introduction">Introduction</h2>
<p>Financial timeseries appear to have a very low signal to noise ratio, where the variance (the power of the noise frequency) can 
be higher than the power of the overall signal.  There are many different ways to quantify the S/N ratio of a financial timeseries, but
will not pursue here.</p>

<p>Price uncertainty is a function of limited information.  If we knew or could deduce
the individual decision making and information state of each market actor, we would could have certainty about the next action 
of each participant.  This would still not be enough to predict future prices far forward, unless we could also model the future states of
exogenous information influencing market actors.  While I am aware of and have been involved in limited modeling of actors in the market (for example
market makers model how groups of traders (clients) or specific clients behave), future event prediction would require a super-human 
AI that could predict our ecosystem.</p>

<p>Given our lack of capability to know or predict the actions of individual actors, we have to take on a stochastic 
view of the market where the following exist:</p>

<ul>
  <li>underlying price direction</li>
  <li>substantial noise added to the price direction by differing views of price discovery (or due to purposeful obfuscation)</li>
  <li>supply and demand dynamics which temporarily disrupt longer term price direction</li>
  <li>shocks and regime change</li>
</ul>

<h2 id="the-label-noise-problem">The label noise problem</h2>
<p>The preponderance of machine learning models are supervised (i.e. labeled classifiers or regressors).  Such models 
take on the form:</p>

\[\hat{y} = f(\vec{X}) + \epsilon\]

<p>where we train the function <code class="language-plaintext highlighter-rouge">f(X)</code> to minimize a loss function and a complexity tradeoff, for example:</p>

\[\min_{\theta} \sum_{i} { L(y_i,f(\vec{x_i},\theta)) + \lambda ||x||_1 }\]

<p>where \(L(y,\hat{y})\) is the loss function and \(\lambda ||x||_1\) is the complexity penalty.  There are many different
formulations of the optimisation.   I recommend this article on <a href="https://heartbeat.fritz.ai/5-regression-loss-functions-all-machine-learners-should-know-4fb140e9d4b0">loss functions</a>
as an overview on the types of loss functions typically deployed.  As for the optimisation, may look quite different 
depending on ML algorithm and structure.  Decision tree models, for example, are quite different from SVM and deep learning in
loss function approach.</p>

<p>If our labels (the labels provided in training) have high error, we have biased the model towards that error.  A 50% error
rate in our labeling would certainly result in a model that is as good as random.   However, much lower error rates
will tend to be enough to derail a ML model:</p>

<ul>
  <li>trading opportunities are often the minority label
    <ul>
      <li>(+1 for enter may be 1/3rd or 1/4 of all labels, in the binary classification case)</li>
    </ul>
  </li>
  <li>certain feature / label pairs may be emphasized much more strongly in the model
    <ul>
      <li>(for example due to higher variance of particular feature vectors)</li>
    </ul>
  </li>
  <li>features have their own noise, contributing to degradation of OOS model performance</li>
</ul>

<h3 id="label-noise-digging-in">Label Noise: Digging In</h3>
<p>Lets get a sense of the error presented in daily returns relative to a smooth projection of forward returns in our prior
example.   Recall that our labels were based on the 5 day return being &gt;= 50bps.  To simplify the analysis, lets consider
the noise in daily returns relative to smoothed forward cumulative returns (in bps).</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">...</span>
<span class="n">prices</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">Series</span><span class="p">(</span><span class="n">bars</span><span class="p">[</span><span class="s">"Adj Close"</span><span class="p">].</span><span class="n">values</span><span class="p">.</span><span class="n">flatten</span><span class="p">())</span>
<span class="n">cumr</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">log</span><span class="p">(</span><span class="n">prices</span> <span class="o">/</span> <span class="n">prices</span><span class="p">.</span><span class="n">iloc</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span> <span class="o">*</span> <span class="mf">1e4</span>

<span class="k">def</span> <span class="nf">smooth</span> <span class="p">(</span><span class="n">y</span><span class="p">:</span> <span class="n">pd</span><span class="p">.</span><span class="n">Series</span><span class="p">,</span> <span class="n">bandwidth</span><span class="p">:</span> <span class="nb">float</span><span class="p">):</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">cumr</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
    <span class="k">return</span> <span class="n">pd</span><span class="p">.</span><span class="n">Series</span><span class="p">(</span><span class="n">lowess</span><span class="p">(</span><span class="n">y</span><span class="p">.</span><span class="n">values</span><span class="p">.</span><span class="n">flatten</span><span class="p">(),</span> <span class="n">x</span><span class="p">,</span> <span class="n">frac</span><span class="o">=</span><span class="n">bandwidth</span><span class="p">)[:,</span><span class="mi">1</span><span class="p">].</span><span class="n">flatten</span><span class="p">())</span>

<span class="n">smoothr1</span> <span class="o">=</span> <span class="n">smooth</span><span class="p">(</span><span class="n">cumr</span><span class="p">,</span> <span class="mf">0.002</span><span class="p">)</span>
<span class="n">smoothr2</span> <span class="o">=</span> <span class="n">smooth</span><span class="p">(</span><span class="n">cumr</span><span class="p">,</span> <span class="mf">0.01</span><span class="p">)</span>
<span class="n">error1</span> <span class="o">=</span> <span class="n">cumr</span> <span class="o">-</span> <span class="n">smoothr1</span>
<span class="n">error2</span> <span class="o">=</span> <span class="n">cumr</span> <span class="o">-</span> <span class="n">smoothr2</span>

<span class="n">stat1</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nb">abs</span><span class="p">(</span><span class="n">error1</span><span class="p">).</span><span class="n">median</span><span class="p">()</span>
<span class="n">stat2</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nb">abs</span><span class="p">(</span><span class="n">error2</span><span class="p">).</span><span class="n">median</span><span class="p">()</span>
<span class="p">...</span>
</code></pre></div></div>

<p><img src="/assets/2020-07-28/return-error.png" alt="Return Error" /></p>

<p>The low smoothing (smoothing1) is fairly light smoothing with an approximate period in line with the 5day return direction.
Note that the median error is 38bps and that 39% of these errors exceed 50bps (whereas the remaining 61% are below 50bps).
An error rate this high will derail any model targeting return based labels.</p>

<h3 id="feature-noise-digging-in">Feature Noise: Digging In</h3>
<p>As you can imagine, features based on prices can also exhibit similar noise.  Let‚Äôs cheat a bit and see what happens if we
smooth prices used for both features and labels, removing the label and feature noise.   Note that this will not result in a valid strategy
as the smoothing used above is global and therefore has a lookahead bias.   However the difference in OOS outcome should
be significant if we denoise.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">...</span>
<span class="n">smoothr1</span> <span class="o">=</span> <span class="n">smooth</span><span class="p">(</span><span class="n">cumr</span><span class="p">,</span> <span class="mf">0.001</span><span class="p">)</span>
<span class="n">smoothr2</span> <span class="o">=</span> <span class="n">smooth</span><span class="p">(</span><span class="n">cumr</span><span class="p">,</span> <span class="mf">0.002</span><span class="p">)</span>
<span class="n">smoothr3</span> <span class="o">=</span> <span class="n">smooth</span><span class="p">(</span><span class="n">cumr</span><span class="p">,</span> <span class="mf">0.005</span><span class="p">)</span>

<span class="c1"># create features
</span><span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">DataFrame</span><span class="p">({</span>
    <span class="s">"rsi.1"</span><span class="p">:</span> <span class="n">talib</span><span class="p">.</span><span class="n">RSI</span><span class="p">(</span><span class="n">smoothr1</span><span class="p">,</span> <span class="n">timeperiod</span><span class="o">=</span><span class="mi">14</span><span class="p">),</span>
    <span class="s">"rsi.2"</span><span class="p">:</span> <span class="n">talib</span><span class="p">.</span><span class="n">RSI</span><span class="p">(</span><span class="n">smoothr2</span><span class="p">,</span> <span class="n">timeperiod</span><span class="o">=</span><span class="mi">14</span><span class="p">),</span>
    <span class="s">"rsi.3"</span><span class="p">:</span> <span class="n">talib</span><span class="p">.</span><span class="n">RSI</span><span class="p">(</span><span class="n">smoothr3</span><span class="p">,</span> <span class="n">timeperiod</span><span class="o">=</span><span class="mi">14</span><span class="p">),</span>
    <span class="p">...</span>
<span class="p">})</span>

<span class="c1"># create { 0, 1 } labels, where 1 means smoothed 1day return &gt;= 20bps
</span><span class="n">df</span><span class="p">[</span><span class="s">"rfwd"</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s">"roc1.1"</span><span class="p">].</span><span class="n">shift</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
<span class="n">df</span><span class="p">[</span><span class="s">"label"</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">df</span><span class="p">.</span><span class="n">rfwd</span> <span class="o">&gt;=</span> <span class="mf">0.20</span><span class="p">)</span> <span class="o">*</span> <span class="mf">1.0</span>
<span class="p">...</span>

<span class="n">icut</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">df</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="mf">0.70</span><span class="p">)</span>
<span class="n">training</span> <span class="o">=</span> <span class="n">df</span><span class="p">.</span><span class="n">iloc</span><span class="p">[:</span><span class="n">icut</span><span class="p">].</span><span class="n">dropna</span><span class="p">()</span>
<span class="n">testing</span> <span class="o">=</span> <span class="n">df</span><span class="p">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">icut</span><span class="p">:].</span><span class="n">dropna</span><span class="p">()</span>

<span class="n">clf</span> <span class="o">=</span> <span class="n">RandomForestClassifier</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_jobs</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">clf</span><span class="p">.</span><span class="n">fit</span> <span class="p">(</span><span class="n">training</span><span class="p">[</span><span class="n">features</span><span class="p">],</span> <span class="n">training</span><span class="p">.</span><span class="n">label</span><span class="p">)</span>

<span class="n">pred_train</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">training</span><span class="p">[</span><span class="n">features</span><span class="p">])</span>
<span class="n">pred_test</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">testing</span><span class="p">[</span><span class="n">features</span><span class="p">])</span>
</code></pre></div></div>
<p>In this scenario we now get an OOS precision of 72% instead of 47% precision in the prior model.  If it were not for the lookahead bias
in smoothing, this would lead to a very profitable strategy.</p>

<table>
  <thead>
    <tr>
      <th>/</th>
      <th>Positive</th>
      <th>Negative</th>
      <th>Accuracy</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Positive</td>
      <td>408</td>
      <td>159</td>
      <td>72%</td>
    </tr>
    <tr>
      <td>Negative</td>
      <td>209</td>
      <td>851</td>
      <td>80%</td>
    </tr>
  </tbody>
</table>

<h2 id="dodging-the-problem">Dodging the problem</h2>
<p>Without solving the noise problem there are ways of avoiding the ‚Äúlabeling problem‚Äù, for example:</p>

<ul>
  <li>unsupervised learning (without a label objective)
    <ul>
      <li>examples of this include HMM, KNN, etc</li>
    </ul>
  </li>
  <li>regression
    <ul>
      <li>this is quasi-labelled in that maps to (noisy) outcomes, but does not have quantized outcomes as with classification</li>
    </ul>
  </li>
  <li>EA (genetic algorithms and genetic programs)
    <ul>
      <li>one solves for an overall performance objective rather than targeting labels</li>
    </ul>
  </li>
</ul>

<p>Of course using any of these means one has reframed a problem in a very different way.  There are many times where
classification makes the most sense, and the above are not especially applicable.</p>

<h2 id="partial-solutions">Partial Solutions</h2>
<p>There has been some limited exploration of noisy labels in the literature (here is an <a href="https://papers.nips.cc/paper/5073-learning-with-noisy-labels.pdf">example</a>), 
though one rarely sees direct support for class-conditional noise loss functions in libraries such as <code class="language-plaintext highlighter-rouge">sklearn</code>.
The loss approach discussed in the linked paper assumes a consistent noise distribution across the labels.  Whereas 
we should expect that our label noise varies across time with market volatility (the conditional noise distribution changes
over time).</p>

<p>There are some heuristic approaches one can take:</p>

<ul>
  <li>the most obvious, remove noise with pre-smoothing
    <ul>
      <li>Given that noise exists in both labels and features (where removing feature noise often cannot be removed without lookahead
bias or alternatively lag), pre-smoothing only addresses label noise and not feature noise</li>
    </ul>
  </li>
  <li>use a classification algorithm that allows weights, weighting the samples based on expected label noise
    <ul>
      <li>this limits the ML algorithms we can use and also means our model fails to predict well in higher volatility situations</li>
      <li>the lower weight samples may also be of more interest in trading, defeating the purpose</li>
    </ul>
  </li>
</ul>

<p>A more interesting heuristic approach (which I have used successfully) attempts to circumvent feature and label noise simultaneously by using a 2 stage approach
to classification:</p>

<ul>
  <li>use a model trained on original labels to generate new labels
    <ul>
      <li>some % of these labels will be our given labels and some % relabelled to an alternative class</li>
      <li>retain TP, TN, and FN, however remap FP ‚Üí 0</li>
      <li>do this on N folds within the training set until the whole training set is relabeled</li>
    </ul>
  </li>
  <li>train a new model based on the generated / modified labels
    <ul>
      <li>I found that this improves precision, removing the impedance where TP‚Äôs could not be achieved with the original labeling</li>
    </ul>
  </li>
</ul>

<h3 id="sample-code-for-resampling-labels-with-a-rf">Sample Code (for resampling labels with a RF)</h3>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">relabeled</span><span class="p">(</span><span class="n">X</span><span class="p">:</span> <span class="n">pd</span><span class="p">.</span><span class="n">DataFrame</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="n">pd</span><span class="p">.</span><span class="n">Series</span><span class="p">,</span> <span class="n">nfold</span> <span class="o">=</span> <span class="mi">10</span><span class="p">):</span>
    <span class="n">w</span> <span class="o">=</span> <span class="n">X</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">cuts</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="n">w</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span><span class="n">w</span><span class="o">/</span><span class="n">nfold</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">int</span><span class="p">)</span>
    <span class="n">cuts</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">w</span>
    
    <span class="n">newlabels</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">w</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="n">cuts</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">-</span><span class="mi">1</span><span class="p">):</span>
        <span class="n">xtesting</span> <span class="o">=</span> <span class="n">X</span><span class="p">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">cuts</span><span class="p">[</span><span class="n">i</span><span class="p">]:</span><span class="n">cuts</span><span class="p">[</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">]]</span>
        <span class="n">ytesting</span> <span class="o">=</span> <span class="n">y</span><span class="p">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">cuts</span><span class="p">[</span><span class="n">i</span><span class="p">]:</span><span class="n">cuts</span><span class="p">[</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">]]</span>
        
        <span class="n">xtraining</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">concat</span><span class="p">([</span><span class="n">X</span><span class="p">.</span><span class="n">iloc</span><span class="p">[:</span><span class="n">cuts</span><span class="p">[</span><span class="n">i</span><span class="p">]],</span> <span class="n">X</span><span class="p">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">cuts</span><span class="p">[</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">]:]])</span>
        <span class="n">ytraining</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">concat</span><span class="p">([</span><span class="n">y</span><span class="p">.</span><span class="n">iloc</span><span class="p">[:</span><span class="n">cuts</span><span class="p">[</span><span class="n">i</span><span class="p">]],</span> <span class="n">y</span><span class="p">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">cuts</span><span class="p">[</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">]:]])</span>
        
        <span class="n">clf</span> <span class="o">=</span> <span class="n">RandomForestClassifier</span> <span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_jobs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">class_weight</span><span class="o">=</span><span class="s">'balanced'</span><span class="p">)</span>
        <span class="n">model</span> <span class="o">=</span> <span class="n">clf</span><span class="p">.</span><span class="n">fit</span> <span class="p">(</span><span class="n">xtraining</span><span class="p">,</span> <span class="n">ytraining</span><span class="p">)</span>
        <span class="n">ynew</span> <span class="o">=</span> <span class="n">clf</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">xtesting</span><span class="p">)</span>
        
        <span class="n">TP</span> <span class="o">=</span> <span class="p">(</span><span class="n">ynew</span> <span class="o">==</span> <span class="mi">1</span><span class="p">)</span> <span class="o">&amp;</span> <span class="p">(</span><span class="n">ytesting</span> <span class="o">==</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">FP</span> <span class="o">=</span> <span class="p">(</span><span class="n">ynew</span> <span class="o">==</span> <span class="mi">1</span><span class="p">)</span> <span class="o">&amp;</span> <span class="p">(</span><span class="n">ytesting</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span>
        <span class="n">FN</span> <span class="o">=</span> <span class="p">(</span><span class="n">ynew</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span> <span class="o">&amp;</span> <span class="p">(</span><span class="n">ytesting</span> <span class="o">==</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">TN</span> <span class="o">=</span> <span class="p">(</span><span class="n">ynew</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span> <span class="o">&amp;</span> <span class="p">(</span><span class="n">ytesting</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span>
        
        <span class="n">precision</span> <span class="o">=</span> <span class="n">TP</span><span class="p">.</span><span class="nb">sum</span><span class="p">()</span> <span class="o">/</span> <span class="p">(</span><span class="n">TP</span><span class="p">.</span><span class="nb">sum</span><span class="p">()</span> <span class="o">+</span> <span class="n">FP</span><span class="p">.</span><span class="nb">sum</span><span class="p">())</span> <span class="o">*</span> <span class="mf">100.0</span>
        <span class="k">print</span> <span class="p">(</span><span class="s">"[%d/%d] precision: %1.1f%%"</span> <span class="o">%</span> <span class="p">(</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span> <span class="n">cuts</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">precision</span><span class="p">))</span>
        
        <span class="n">ynew</span><span class="p">[</span><span class="n">FP</span><span class="p">]</span> <span class="o">=</span> <span class="mf">0.0</span>
        <span class="n">newlabels</span><span class="p">[</span><span class="n">cuts</span><span class="p">[</span><span class="n">i</span><span class="p">]:</span><span class="n">cuts</span><span class="p">[</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">]]</span> <span class="o">=</span> <span class="n">ynew</span>
        
    <span class="k">return</span> <span class="n">newlabels</span>
</code></pre></div></div>
<h3 id="alternatives">Alternatives</h3>
<p>There are a variety of methods described here <a href="https://l7.curtisnorthcutt.com/confident-learning">‚ÄúConfident Learning‚Äù</a>.  I
have not found these methods to be as useful as the heuristic above, probably because of the focus on labels versus the
combined behavior of noisy features and noisy labels.</p>

<h2 id="conclusions">Conclusions</h2>
<p>I am still very unsatisfied with the handling of this problem.  I think the optimal solution will lie in creating
machine learning algorithms that explicitly allow one to parameterize feature and label noise across the data 
set.  Will think further on how this should be structured.   Would love to get pointers from anyone that has
dealt with these problems.</p>

<p>In the next post I will try to tackle lack of feature vector independence.</p>

:ET